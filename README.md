# Data-Analyzer

### Hypothesis 
Given the tendencies for micro-aggression towards Asian-Americans today, there should be an abundance of evidences demonstrating implicit bias against Asian Americans on the popular communication platform Twitter.

### Methodology
Using tweepy I was able to scrape a myriad of tweets matching certain queries from Twitter, but throughout the development process there were many setbacks that needed to be addressed before the data can be considered reliable. Due to Twitter’s effort to promote free speech there was an abundance of advertising tweets promoting pornography based on race. As a result, I identified certain keywords to prune out some unwanted tweets from the result tree in effort to prevent the outcome from being skewed due to these unsolicited advertising spams. However, the bigger problem arose after fixing said problem, Twitter is inundated with repetitious spam tweets that again would skew the outcome to favor tweets by bots and would not be reflective of the community overall. As a result, I employed a dictionary of tweets to ensure repeated tweets are excluded from the data which introduced yet another problem of inaccurate scrape count, but this was easily fixed by using a counter for specifically unique tweets. After figuring out how to scrape meaningful data, the last step before analyzing the information gathered is using regular expressions to parse raw Twitter API output into something readable. The method I have elected to analyze the data is by using the spaCy API to tag each word within the tweets with their part of speech and find the most used noun and adjective respectively. The reason I decided to pursuit this route instead of simply looking for the most repeated word is merely finding the most repeated word will result in merely conjunctions (and, or but), articles (the), and pronouns (he or she) since they will easily be the most repeated words while providing me with nothing substantial. Moreover, when finding most occurred words I purposely exclude the word I used to query the data since it’s obvious that whatever I used to query the data will be present in every single tweet making it the most common word. 

### Data
![](https://github.com/alexander871015/AsianPam214-Data-Analyzer/blob/master/Pics/adjective_count.PNG)
![](https://github.com/alexander871015/AsianPam214-Data-Analyzer/blob/master/Pics/noun_count.PNG)

### Analysis
As seen from the data shown above, my methodology did not corroborate with my hypothesis, none of the repeated words suggest an implicit bias on Twitter. Notice the reoccurring words all make sense, for example, Asian is a word to describe a specific race, so it makes sense that they are often grouped with words like black and white because said tweet is most likely discussing racial issues. With that said, it is important to note that in many of the analysis the most commonly occurred words does not even occur that much in relation to how many tweets are analyzed. For example, out of 25,225 tweets about Filipino the most common adjective is “my” and it only occurs 585 times, occurring in only about 2.3% of the tweets. It is by no means representative of the remainder 98% of the tweets. The fact that the words must be spelled the same and be the exact same word serves as a major limitation of this study, for my program is unable to analyze considering different forms without adding major complexity that is difficult to surmount. For example, if I want to group singular and plural words together, I can’t simply check if two words are the same if ‘s’ is appended to the end of the word eg. Enemys and enemy. Moreover, there’s no way to deal with words that are misspelled, for example, if someone misspelled ‘smart’ as say ‘smrt’ or use a synonym such as ‘intelligent’ they should be counted as the same but that’s not possible given the methodology.

### Conclusion
Although the data suggests that there is no micro-aggression towards Asians on Twitter, due to immense limitations of the methodology as explained in the analysis section the result is rendered inconclusive. There are too much room for error, and the analysis is not sophisticated enough to extrapolate a convincing conclusion.

### Directions for Future Studies
As suggested in the conclusion more research needs to be done to improve the methodology in order to get conclusive outcome. In order to consider both words and their synonyms on will need an API for synonyms of words to make each entry a set of words instead of a single word. This would, however, greatly reduce the efficiency of the code since each word will need to be checked against every single current word and their synonyms instead of just current words. In turns of figuring out how to deal with words of different forms, it will require complex machine learning algorithm to figure out both different forms of words and identify misspelled words.  
